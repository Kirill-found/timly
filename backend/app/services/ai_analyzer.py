"""
AI —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ v3.0
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI GPT-4o –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

v3.0 Changes:
- Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏ (–±–æ–Ω—É—Å–Ω—ã–µ –±–∞–ª–ª—ã)
- Skill Provenance (–Ω–∞–≤—ã–∫ —É–∫–∞–∑–∞–Ω vs –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è)
- Gross/Net –∑–∞—Ä–ø–ª–∞—Ç–∞
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ (–¥–µ–∫—Ä–µ—Ç, —Å–∞–±–±–∞—Ç–∏–∫–∞–ª)
- Junior mode (–ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç—ã)
- –°–≤–µ—Ç–æ—Ñ–æ—Ä –≤–µ—Ä–¥–∏–∫—Ç
- –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –∫–∞–∫ hard filter
"""
import json
import time
import hashlib
from typing import Dict, Any, List, Optional
import logging
import openai
import redis
from openai import AsyncOpenAI

from app.config import settings
from app.utils.exceptions import AIAnalysisError

logger = logging.getLogger(__name__)

# Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –±–æ–Ω—É—Å–Ω—ã—Ö –±–∞–ª–ª–æ–≤
TIER1_COMPANIES = [
    # Tech Giants
    "—è–Ω–¥–µ–∫—Å", "yandex", "—Å–±–µ—Ä", "sber", "—Ç–∏–Ω—å–∫–æ—Ñ—Ñ", "tinkoff", "t-bank",
    "–≤–∫", "vk", "mail.ru", "–º–µ–π–ª", "avito", "–∞–≤–∏—Ç–æ", "ozon", "–æ–∑–æ–Ω",
    "wildberries", "–≤–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑", "wb", "lamoda", "–ª–∞–º–æ–¥–∞",
    # International
    "google", "–≥—É–≥–ª", "meta", "facebook", "amazon", "microsoft", "apple",
    "netflix", "uber", "spotify", "airbnb", "stripe", "shopify",
    # Russian Tech
    "–∫–∞—Å–ø–µ—Ä—Å–∫–∏–π", "kaspersky", "positive technologies", "–ø–æ–∑–∏—Ç–∏–≤",
    "jetbrains", "–¥–∂–µ—Ç–±—Ä–µ–π–Ω—Å", "miro", "–º–∏—Ä–æ", "notion",
    # Fintech
    "revolut", "wise", "n26", "–∞–ª—å—Ñ–∞-–±–∞–Ω–∫", "alfa-bank", "—Ä–∞–π—Ñ—Ñ–∞–π–∑–µ–Ω",
    # E-commerce
    "aliexpress", "–∞–ª–∏—ç–∫—Å–ø—Ä–µ—Å—Å", "cdek", "—Å–¥—ç–∫", "boxberry",
    # Consulting
    "mckinsey", "bcg", "bain", "deloitte", "pwc", "kpmg", "ey",
]


class AIAnalyzer:
    """
    AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ v3.0
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
    """

    def __init__(self):
        # –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI SDK
        logging.getLogger("openai").setLevel(logging.WARNING)
        logging.getLogger("httpx").setLevel(logging.WARNING)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        import httpx
        http_client = None
        if hasattr(settings, 'OPENAI_PROXY_URL') and settings.OPENAI_PROXY_URL:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏ –¥–ª—è OpenAI: {settings.OPENAI_PROXY_URL}")
            http_client = httpx.AsyncClient(
                proxies={
                    "http://": settings.OPENAI_PROXY_URL,
                    "https://": settings.OPENAI_PROXY_URL
                },
                timeout=90.0
            )

        self.client = AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY,
            http_client=http_client
        )
        self.model = settings.OPENAI_MODEL

        # Redis –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        try:
            self.cache = redis.Redis.from_url(
                settings.REDIS_URL,
                decode_responses=True,
                socket_connect_timeout=5
            )
            self.cache.ping()
        except Exception as e:
            logger.warning(f"Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ: {e}")
            self.cache = None

    def _get_cache_key(self, vacancy_data: Dict, resume_data: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫–µ—à–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        vacancy_hash = hashlib.md5(
            json.dumps(vacancy_data, sort_keys=True).encode()
        ).hexdigest()[:8]
        resume_hash = hashlib.md5(
            json.dumps(resume_data, sort_keys=True).encode()
        ).hexdigest()[:8]
        return f"analysis:v3:{vacancy_hash}:{resume_hash}"

    def _get_cached_analysis(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫–µ—à–∞"""
        if not self.cache:
            return None
        try:
            cached = self.cache.get(cache_key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
        return None

    def _cache_analysis(self, cache_key: str, result: Dict[str, Any], ttl: int = 86400):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–µ—à (24 —á–∞—Å–∞)"""
        if not self.cache:
            return
        try:
            self.cache.setex(cache_key, ttl, json.dumps(result, ensure_ascii=False))
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫–µ—à: {e}")

    def _detect_tier1_companies(self, experience_text: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Tier-1 –∫–æ–º–ø–∞–Ω–∏–π –≤ –æ–ø—ã—Ç–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        found = []
        text_lower = experience_text.lower()
        for company in TIER1_COMPANIES:
            if company in text_lower:
                found.append(company)
        return list(set(found))

    def _create_analysis_prompt(
        self,
        vacancy_data: Dict[str, Any],
        resume_data: Dict[str, Any]
    ) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞ v3.0

        –í–∫–ª—é—á–∞–µ—Ç:
        - Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏
        - Skill Provenance
        - Gross/Net –∑–∞—Ä–ø–ª–∞—Ç–∞
        - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤
        - Junior mode
        - –°–≤–µ—Ç–æ—Ñ–æ—Ä
        """
        # –ü–∞—Ä—Å–∏–Ω–≥ resume_data –µ—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
        if isinstance(resume_data, str):
            try:
                resume_data = json.loads(resume_data)
            except json.JSONDecodeError:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å resume_data –∫–∞–∫ JSON")
                resume_data = {}

        if not resume_data or not isinstance(resume_data, dict):
            resume_data = {}

        # === –ü–û–õ–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã (–±–µ–∑ –æ–±—Ä–µ–∑–∫–∏!) ===
        experience_list = resume_data.get('experience', [])
        experience_text = ""
        companies_list = []

        if experience_list and isinstance(experience_list, list):
            for idx, exp in enumerate(experience_list[:5]):  # –î–æ 5 –º–µ—Å—Ç —Ä–∞–±–æ—Ç—ã
                if isinstance(exp, dict):
                    company = exp.get('company', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ') if isinstance(exp.get('company'), dict) else '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
                    companies_list.append(company)
                    position = exp.get('position', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')

                    # –î–∞—Ç—ã
                    start_year = exp.get('start', {}).get('year', '') if isinstance(exp.get('start'), dict) else ''
                    start_month = exp.get('start', {}).get('month', '') if isinstance(exp.get('start'), dict) else ''
                    end_year = exp.get('end', {}).get('year', '–Ω.–≤.') if exp.get('end') and isinstance(exp.get('end'), dict) else '–Ω.–≤.'
                    end_month = exp.get('end', {}).get('month', '') if exp.get('end') and isinstance(exp.get('end'), dict) else ''

                    period = f"{start_month or ''}/{start_year}" if start_month else str(start_year)
                    period += f" - {end_month or ''}/{end_year}" if end_month else f" - {end_year}"

                    # –ü–û–õ–ù–û–ï –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è Skill Provenance!)
                    description = exp.get('description', '') or ''

                    experience_text += f"""
### –ú–µ—Å—Ç–æ —Ä–∞–±–æ—Ç—ã #{idx + 1}
**–ö–æ–º–ø–∞–Ω–∏—è:** {company}
**–î–æ–ª–∂–Ω–æ—Å—Ç—å:** {position}
**–ü–µ—Ä–∏–æ–¥:** {period}
**–ó–∞–¥–∞—á–∏ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:**
{description if description else '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}
"""

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏
        tier1_found = self._detect_tier1_companies(experience_text + ' '.join(companies_list))

        # === –ù–∞–≤—ã–∫–∏ ===
        skills_list = []
        for skill in resume_data.get('skill_set', []):
            if isinstance(skill, dict):
                skills_list.append(skill.get('name', ''))
            elif isinstance(skill, str):
                skills_list.append(skill)
        skills_text = ', '.join(skills_list) if skills_list else '–ù–µ —É–∫–∞–∑–∞–Ω—ã'

        # === –†–∞–∑–¥–µ–ª "–û–±–æ –º–Ω–µ" (–ü–û–õ–ù–´–ô, –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏) ===
        about_text = resume_data.get('skills', '') or resume_data.get('about', '') or ''

        # === –°–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ ===
        cover_letter = resume_data.get('cover_letter', '') or resume_data.get('message', '') or ''

        # === –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ ===
        education_list = resume_data.get('education', [])
        education_text = ""
        if education_list and isinstance(education_list, list):
            for edu in education_list[:3]:
                if isinstance(edu, dict):
                    name = edu.get('name', '')
                    result = edu.get('result', '')
                    year = edu.get('year', '')
                    education_text += f"\n- {name} ({year}): {result}" if result else f"\n- {name} ({year})"

        # === –Ø–∑—ã–∫–∏ ===
        languages = resume_data.get('language', [])
        languages_text = ""
        if languages and isinstance(languages, list):
            for lang in languages:
                if isinstance(lang, dict):
                    name = lang.get('name', '')
                    level = lang.get('level', {}).get('name', '') if isinstance(lang.get('level'), dict) else ''
                    languages_text += f"\n- {name}: {level}"

        # === –ó–∞—Ä–ø–ª–∞—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ ===
        candidate_salary = (resume_data.get('salary') or {}).get('amount', 0)
        candidate_currency = (resume_data.get('salary') or {}).get('currency', 'RUR')

        # === –û–±—â–∏–π –æ–ø—ã—Ç ===
        total_months = (resume_data.get('total_experience') or {}).get('months', 0)
        total_years = total_months // 12
        total_months_remainder = total_months % 12

        # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ ===
        vacancy_title = vacancy_data.get('title', '').lower()
        is_junior = any(word in vacancy_title for word in ['junior', '–¥–∂—É–Ω–∏–æ—Ä', '—Å—Ç–∞–∂–µ—Ä', 'trainee', 'intern'])
        is_it_role = any(word in vacancy_title for word in [
            'developer', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', 'engineer', 'devops', 'qa', '—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫',
            'frontend', 'backend', 'fullstack', 'data', 'analyst', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'product', '–ø—Ä–æ–¥–∞–∫—Ç',
            '–¥–∏–∑–∞–π–Ω–µ—Ä', 'designer', 'marketing', '–º–∞—Ä–∫–µ—Ç–æ–ª–æ–≥'
        ])

        # === –ü–û–õ–ù–û–ï –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ (–ë–ï–ó –û–ë–†–ï–ó–ö–ò!) ===
        vacancy_description = vacancy_data.get('description', '') or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'
        vacancy_skills = ', '.join(vacancy_data.get('key_skills', [])) or '–ù–µ —É–∫–∞–∑–∞–Ω—ã'

        # === –ó–∞—Ä–ø–ª–∞—Ç–Ω–∞—è –≤–∏–ª–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ ===
        salary_from = vacancy_data.get('salary_from', 0) or 0
        salary_to = vacancy_data.get('salary_to', 0) or 0
        salary_currency = vacancy_data.get('currency', 'RUB')

        return f"""## –†–û–õ–¨
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π IT-—Ä–µ–∫—Ä—É—Ç–µ—Ä —Å 10+ –ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º. –¢—ã –ø—Ä–æ–≤–æ–¥–∏—à—å –ø–µ—Ä–≤–∏—á–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ —Ä–µ–∑—é–º–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ù–ï –æ—Ç—Å–µ—è—Ç—å —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (false negative —Ö—É–∂–µ false positive).
–¢—ã –¥–∞—ë—à—å —Ä–µ–∫—Ä—É—Ç–µ—Ä—É **—Ñ–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É**, —á—Ç–æ–±—ã –æ–Ω –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ.

## –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ò–ù–¶–ò–ü–´

### 1. SKILL PROVENANCE (–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–≤—ã–∫–æ–≤)
–ù–∞–≤—ã–∫ –≤ —Å–ø–∏—Å–∫–µ "Skills" ‚â† —Ä–µ–∞–ª—å–Ω—ã–π –Ω–∞–≤—ã–∫!
- –ò—â–∏ –î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í–ê –Ω–∞–≤—ã–∫–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –∑–∞–¥–∞—á –Ω–∞ –º–µ—Å—Ç–∞—Ö —Ä–∞–±–æ—Ç—ã
- –ï—Å–ª–∏ –Ω–∞–≤—ã–∫ —É–∫–∞–∑–∞–Ω –≤ Skills, –Ω–æ –ù–ï —É–ø–æ–º—è–Ω—É—Ç –Ω–∏ –≤ –æ–¥–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã ‚Üí "unverified_skills"
- –ï—Å–ª–∏ –Ω–∞–≤—ã–∫ —É–∫–∞–∑–∞–Ω –ò –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω –∑–∞–¥–∞—á–∞–º–∏ ‚Üí "verified_skills"

### 2. TIER-1 –ö–û–ú–ü–ê–ù–ò–ò (–ö–∞—á–µ—Å—Ç–≤–æ —à–∫–æ–ª—ã)
–û–ø—ã—Ç –≤ —Ç–æ–ø–æ–≤—ã—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö = –±–æ–Ω—É—Å –∫ –æ—Ü–µ–Ω–∫–µ:
**Tier-1 IT:** –Ø–Ω–¥–µ–∫—Å, –°–±–µ—Ä, –¢–∏–Ω—å–∫–æ—Ñ—Ñ, VK, –ê–≤–∏—Ç–æ, Ozon, Wildberries, Google, Meta, Amazon
**Tier-1 –ö–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥:** McKinsey, BCG, Bain, Deloitte, PwC, KPMG
{"**–í —Ä–µ–∑—é–º–µ –Ω–∞–π–¥–µ–Ω—ã Tier-1:** " + ", ".join(tier1_found) if tier1_found else "**Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã**"}

### 3. –ó–ê–†–ü–õ–ê–¢–ê: GROSS vs NET
- –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –æ–±—ã—á–Ω–æ —É–∫–∞–∑—ã–≤–∞—é—Ç NET (–Ω–∞ —Ä—É–∫–∏)
- –í–∞–∫–∞–Ω—Å–∏–∏ —á–∞—Å—Ç–æ —É–∫–∞–∑—ã–≤–∞—é—Ç GROSS (–¥–æ –Ω–∞–ª–æ–≥–æ–≤)
- NET √ó 1.15 ‚âà GROSS (—É—á–∏—Ç—ã–≤–∞–π 13-15% –Ω–∞–ª–æ–≥)
- –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 20% –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ ‚Äî —ç—Ç–æ –û–ö

### 4. –ü–†–û–ë–ï–õ–´ –í –ö–ê–†–¨–ï–†–ï (Context Matters)
–ù–ï –Ω–∞–∫–∞–∑—ã–≤–∞–π –∑–∞:
- –î–µ–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç–ø—É—Å–∫ (1-3 –≥–æ–¥–∞ ‚Äî –Ω–æ—Ä–º–∞)
- –°–∞–±–±–∞—Ç–∏–∫–∞–ª/burnout recovery (–¥–æ 1 –≥–æ–¥–∞)
- –ü–µ—Ä–µ–µ–∑–¥ –≤ –¥—Ä—É–≥—É—é —Å—Ç—Ä–∞–Ω—É
- –£—á—ë–±–∞/MBA/–∫—É—Ä—Å—ã
–ü–æ–º–µ—á–∞–π –∫–∞–∫ risk –¢–û–õ–¨–ö–û –Ω–µ–æ–±—ä—è—Å–Ω—ë–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã >12 –º–µ—Å—è—Ü–µ–≤

### 5. JUNIOR MODE {"(–ê–ö–¢–ò–í–ï–ù)" if is_junior else "(–ù–ï –ê–ö–¢–ò–í–ï–ù)"}
{"–î–ª—è –¥–∂—É–Ω–∏–æ—Ä-–ø–æ–∑–∏—Ü–∏–π: –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç—ã, GitHub, –∫—É—Ä—Å—ã = —Ä–µ–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º 0.5" if is_junior else ""}

### 6. –ß–ê–°–¢–ê–Ø –°–ú–ï–ù–ê –†–ê–ë–û–¢–´
{"IT/–°—Ç–∞—Ä—Ç–∞–ø—ã: —Å–º–µ–Ω–∞ –∫–∞–∂–¥—ã–µ 1.5-2 –≥–æ–¥–∞ ‚Äî –ù–û–†–ú–ê, –Ω–µ red flag" if is_it_role else "–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –æ—Ç—Ä–∞—Å–ª–∏: –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç–∞–∂ 3+ –≥–æ–¥–∞ –Ω–∞ –º–µ—Å—Ç–µ"}

---

## –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê

### –í–ê–ö–ê–ù–°–ò–Ø
**–ù–∞–∑–≤–∞–Ω–∏–µ:** {vacancy_data.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}
**–¢—Ä–µ–±—É–µ–º—ã–µ –Ω–∞–≤—ã–∫–∏:** {vacancy_skills}
**–¢—Ä–µ–±—É–µ–º—ã–π –æ–ø—ã—Ç:** {vacancy_data.get('experience', '–ù–µ —É–∫–∞–∑–∞–Ω')}
**–ó–∞—Ä–ø–ª–∞—Ç–Ω–∞—è –≤–∏–ª–∫–∞:** {salary_from} - {salary_to} {salary_currency} {"(–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ GROSS)" if salary_from > 100000 else ""}
**–ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏:**
\"\"\"
{vacancy_description}
\"\"\"

### –ö–ê–ù–î–ò–î–ê–¢
**–§–ò–û:** {resume_data.get('first_name', '')} {resume_data.get('last_name', '')}
**–¢–µ–∫—É—â–∞—è/–ø–æ—Å–ª–µ–¥–Ω—è—è –¥–æ–ª–∂–Ω–æ—Å—Ç—å:** {resume_data.get('title', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}
**–ì–æ—Ä–æ–¥:** {resume_data.get('area', {}).get('name', '–ù–µ —É–∫–∞–∑–∞–Ω') if isinstance(resume_data.get('area'), dict) else '–ù–µ —É–∫–∞–∑–∞–Ω'}
**–û–±—â–∏–π –æ–ø—ã—Ç:** {total_years} –ª–µ—Ç {total_months_remainder} –º–µ—Å
**–ó–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è:** {candidate_salary} {candidate_currency} {"(–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ NET)" if candidate_salary else "–Ω–µ —É–∫–∞–∑–∞–Ω—ã"}
**–ù–∞–≤—ã–∫–∏ –≤ –ø—Ä–æ—Ñ–∏–ª–µ:** {skills_text}
**–Ø–∑—ã–∫–∏:** {languages_text or '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}
**–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:** {education_text or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}

**–û–±–æ –º–Ω–µ:**
\"\"\"
{about_text or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}
\"\"\"

**–°–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ:**
\"\"\"
{cover_letter or '–ù–µ —É–∫–∞–∑–∞–Ω–æ'}
\"\"\"

### –û–ü–´–¢ –†–ê–ë–û–¢–´ (–î–ï–¢–ê–õ–¨–ù–û)
{experience_text or '–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –Ω–µ —É–∫–∞–∑–∞–Ω'}

---

## –ó–ê–î–ê–ù–ò–ï

### –®–ê–ì 1: –ê–ù–ê–õ–ò–ó –ù–ê–í–´–ö–û–í (Chain of Thought)
1. –í—ã–ø–∏—à–∏ —Ç—Ä–µ–±—É–µ–º—ã–µ –Ω–∞–≤—ã–∫–∏ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–≤—ã–∫–∞ –ø—Ä–æ–≤–µ—Ä—å:
   - –ï—Å—Ç—å –≤ —Å–ø–∏—Å–∫–µ Skills –∫–∞–Ω–¥–∏–¥–∞—Ç–∞?
   - –ï—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã? (—Ü–∏—Ç–∞—Ç–∞ –∏–ª–∏ –∑–∞–¥–∞—á–∞)
3. –†–∞–∑–¥–µ–ª–∏ –Ω–∞: verified (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ), unverified (—Ç–æ–ª—å–∫–æ –≤ —Å–ø–∏—Å–∫–µ), missing (–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç)

### –®–ê–ì 2: –û–¶–ï–ù–ö–ê –ü–û –†–£–ë–†–ò–ö–ï (1-5)

**RELEVANCE (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–∏):**
- 5: –ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ + –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ + —É—Ä–æ–≤–Ω—è + Tier-1 –∫–æ–º–ø–∞–Ω–∏—è
- 4: –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏, –≤–æ–∑–º–æ–∂–Ω–æ –¥—Ä—É–≥–∞—è –∏–Ω–¥—É—Å—Ç—Ä–∏—è
- 3: –°–º–µ–∂–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
- 2: –°–ª–∞–±–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –±–æ–ª—å—à–æ–π gap
- 1: –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ

**EXPERTISE (–ì–ª—É–±–∏–Ω–∞ –Ω–∞–≤—ã–∫–æ–≤):**
- 5: 90%+ –Ω–∞–≤—ã–∫–æ–≤ VERIFIED + –∏–∑–º–µ—Ä–∏–º—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
- 4: 70%+ –Ω–∞–≤—ã–∫–æ–≤, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ verified
- 3: 50%+ –Ω–∞–≤—ã–∫–æ–≤, —á–∞—Å—Ç—å unverified
- 2: <50% –Ω–∞–≤—ã–∫–æ–≤ –∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ missing
- 1: –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤

**TRAJECTORY (–ö–∞—Ä—å–µ—Ä–Ω—ã–π —Ä–æ—Å—Ç):**
- 5: –Ø–≤–Ω—ã–π —Ä–æ—Å—Ç (–ø–æ–≤—ã—à–µ–Ω–∏—è, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ scope, —Ä–æ—Å—Ç –∫–æ–º–∞–Ω–¥—ã) + Tier-1
- 4: –°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏
- 3: –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –∫–∞—Ä—å–µ—Ä–∞, –±–µ–∑ —è–≤–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞
- 2: Downgrade –∏–ª–∏ —Ö–∞–æ—Ç–∏—á–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
- 1: –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –∏–ª–∏ –æ–≥—Ä–æ–º–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã

**STABILITY (–†–∏—Å–∫–∏):**
- 5: –°—Ç–∞–±–∏–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è, –æ–±—ä—è—Å–Ω–∏–º—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
- 4: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏
- 3: –ï—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è
- 2: Yellow flags (–Ω–µ–æ–±—ä—è—Å–Ω—ë–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, —á–∞—Å—Ç—ã–µ —Å–º–µ–Ω—ã –±–µ–∑ –ª–æ–≥–∏–∫–∏)
- 1: Red flags (–Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –æ–ø—ã—Ç, —è–≤–Ω—ã–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è)

### –®–ê–ì 3: –°–í–ï–¢–û–§–û–†
–ù–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–æ–∫ –æ–ø—Ä–µ–¥–µ–ª–∏ –≤–µ—Ä–¥–∏–∫—Ç:
- üü¢ GREEN: –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ ‚â•4.0 ‚Äî –∑–≤–æ–Ω–∏—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
- üü° YELLOW: –°—Ä–µ–¥–Ω—è—è 3.0-3.9 ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç –∑–µ–ª—ë–Ω—ã—Ö
- üî¥ RED: –°—Ä–µ–¥–Ω—è—è <3.0 ‚Äî –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è

---

## –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON)

{{
    "verdict": "GREEN" | "YELLOW" | "RED",
    "verdict_reason": "–û–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: –≥–ª–∞–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –≤–µ—Ä–¥–∏–∫—Ç–∞",

    "scores": {{
        "relevance": 1-5,
        "expertise": 1-5,
        "trajectory": 1-5,
        "stability": 1-5
    }},

    "skills_analysis": {{
        "verified": ["–Ω–∞–≤—ã–∫1 ‚Äî –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –≤ –ø—Ä–æ–µ–∫—Ç–µ X", "–Ω–∞–≤—ã–∫2 ‚Äî –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω: 3 –≥–æ–¥–∞ –æ–ø—ã—Ç–∞ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ Y"],
        "unverified": ["–Ω–∞–≤—ã–∫3 ‚Äî —É–∫–∞–∑–∞–Ω –≤ Skills, –Ω–æ –Ω–µ —É–ø–æ–º—è–Ω—É—Ç –≤ –æ–ø—ã—Ç–µ"],
        "missing": ["–Ω–∞–≤—ã–∫4 ‚Äî —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ –≤–∞–∫–∞–Ω—Å–∏–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"],
        "bonus": ["–Ω–∞–≤—ã–∫5 ‚Äî –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –Ω–æ –ø–æ–ª–µ–∑–µ–Ω"]
    }},

    "experience_summary": {{
        "total_years": —á–∏—Å–ª–æ,
        "relevant_years": —á–∏—Å–ª–æ,
        "last_position": "–¥–æ–ª–∂–Ω–æ—Å—Ç—å",
        "last_company": "–∫–æ–º–ø–∞–Ω–∏—è",
        "tier1_companies": ["–∫–æ–º–ø–∞–Ω–∏—è1"] –∏–ª–∏ [],
        "career_direction": "growth" | "stable" | "decline" | "pivot"
    }},

    "salary_analysis": {{
        "candidate_net": —á–∏—Å–ª–æ –∏–ª–∏ null,
        "candidate_gross_estimated": —á–∏—Å–ª–æ –∏–ª–∏ null,
        "vacancy_range": "–æ—Ç - –¥–æ",
        "match": "within_range" | "above" | "below" | "unknown",
        "comment": "–∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"
    }},

    "pros": [
        "–ö–û–ù–ö–†–ï–¢–ù–´–ô —Ñ–∞–∫—Ç –∏–∑ —Ä–µ–∑—é–º–µ: —Ü–∏—Ç–∞—Ç–∞ –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ",
        "–ï—â—ë —Ñ–∞–∫—Ç —Å —Ü–∏—Ñ—Ä–∞–º–∏ –µ—Å–ª–∏ –µ—Å—Ç—å"
    ],

    "cons": [
        "–ö–û–ù–ö–†–ï–¢–ù–´–ô gap –∏–ª–∏ —Ä–∏—Å–∫ –∏–∑ —Ä–µ–∑—é–º–µ",
        "–ï—â—ë –º–∏–Ω—É—Å –µ—Å–ª–∏ –µ—Å—Ç—å"
    ],

    "red_flags": ["–¢–æ–ª—å–∫–æ —Å–µ—Ä—å—ë–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"] –∏–ª–∏ [],
    "yellow_flags": ["–¢—Ä–µ–±—É—é—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è –Ω–∞ –∏–Ω—Ç–µ—Ä–≤—å—é"] –∏–ª–∏ [],
    "green_flags": ["Tier-1 –∫–æ–º–ø–∞–Ω–∏—è", "–ò–∑–º–µ—Ä–∏–º—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è"] –∏–ª–∏ [],

    "interview_questions": [
        "–ü–ï–†–°–û–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –≤–æ–ø—Ä–æ—Å –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É gap –∏–∑ —Ä–µ–∑—é–º–µ —ç—Ç–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞",
        "–í–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ–ø—ã—Ç/–ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ —Ä–µ–∑—é–º–µ —ç—Ç–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞",
        "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ unverified –Ω–∞–≤—ã–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å"
    ],

    "summary_for_recruiter": "–û–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–≤—å—é: —Ä–æ–ª—å, X –ª–µ—Ç, –∫–ª—é—á–µ–≤–æ–π –ø–ª—é—Å –∏–ª–∏ —Ä–∏—Å–∫",

    "reasoning": "2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –ø–æ—á–µ–º—É —Ç–∞–∫–æ–π –≤–µ—Ä–¥–∏–∫—Ç, —Å –æ—Ç—Å—ã–ª–∫–æ–π –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ñ–∞–∫—Ç–∞–º –∏–∑ —Ä–µ–∑—é–º–µ"
}}

---

## –ü–†–ê–í–ò–õ–ê –ö–ê–ß–ï–°–¢–í–ê

### –î–ª—è pros/cons:
‚ùå –ü–õ–û–•–û: "–•–æ—Ä–æ—à–∏–π –æ–ø—ã—Ç", "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–∞–≤—ã–∫–∏", "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø—ã—Ç–∞"
‚úÖ –•–û–†–û–®–û: "3 –≥–æ–¥–∞ –≤ –Ø–Ω–¥–µ–∫—Å–µ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ Senior Python", "–ù–µ—Ç –æ–ø—ã—Ç–∞ —Å Kubernetes (—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ –≤–∞–∫–∞–Ω—Å–∏–∏)"

### –î–ª—è interview_questions:
‚ùå –ü–õ–û–•–û: "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ —Å–µ–±–µ", "–ö–∞–∫–∏–µ –≤–∞—à–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã?"
‚úÖ –•–û–†–û–®–û: "–í—ã —É–∫–∞–∑–∞–ª–∏ –æ–ø—ã—Ç —Å Kafka ‚Äî —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ, –∫–∞–∫–æ–π throughput –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏?", "–ü–æ—á–µ–º—É —É—à–ª–∏ –∏–∑ –°–±–µ—Ä–∞ –ø–æ—Å–ª–µ 8 –º–µ—Å—è—Ü–µ–≤?"

### –î–ª—è summary_for_recruiter:
‚ùå –ü–õ–û–•–û: "–ü–æ–¥—Ö–æ–¥—è—â–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è"
‚úÖ –•–û–†–û–®–û: "Senior Python, 6 –ª–µ—Ç, ex-–Ø–Ω–¥–µ–∫—Å, –Ω–æ –Ω–µ—Ç K8s ‚Äî –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥"

### –î–ª—è verified skills:
–ù–∞–≤—ã–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è verified –µ—Å–ª–∏ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã –µ—Å—Ç—å:
- –ü—Ä—è–º–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ + –∑–∞–¥–∞—á–∞ ("—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª API –Ω–∞ FastAPI")
- –ò–∑–º–µ—Ä–∏–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ("–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª –∑–∞–ø—Ä–æ—Å—ã SQL, —É—Å–∫–æ—Ä–∏–ª –≤ 3 —Ä–∞–∑–∞")
- –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ("–≤–Ω–µ–¥—Ä–∏–ª CI/CD –Ω–∞ GitLab")
"""

    def _calculate_composite_score(self, scores: Dict[str, int]) -> Dict[str, Any]:
        """–†–∞—Å—á—ë—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ –∏–∑ –æ—Ü–µ–Ω–æ–∫ 1-5"""
        weights = {
            "relevance": 0.35,
            "expertise": 0.30,
            "trajectory": 0.20,
            "stability": 0.15
        }

        weighted_sum = 0
        for key, weight in weights.items():
            score = scores.get(key, 3)
            weighted_sum += score * weight

        rank_score = int((weighted_sum - 1) * 25)  # 1‚Üí0, 5‚Üí100

        if weighted_sum >= 4.0:
            tier = "A"
        elif weighted_sum >= 3.0:
            tier = "B"
        else:
            tier = "C"

        if rank_score >= 75:
            recommendation = "hire"
        elif rank_score >= 55:
            recommendation = "interview"
        elif rank_score >= 40:
            recommendation = "maybe"
        else:
            recommendation = "reject"

        return {
            "rank_score": rank_score,
            "tier": tier,
            "recommendation": recommendation,
            "weighted_average": round(weighted_sum, 2)
        }

    def _enrich_analysis_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ v3.0"""
        scores = result.get("scores", {})

        if not scores:
            old_score = result.get("score", 50)
            return result

        # –†–∞—Å—á—ë—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        composite = self._calculate_composite_score(scores)

        result["rank_score"] = composite["rank_score"]
        result["tier"] = composite["tier"]
        result["recommendation"] = composite["recommendation"]

        # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        result["score"] = composite["rank_score"]

        # –†–∞—Å—á—ë—Ç match_percent –Ω–∞ –±—ç–∫–µ–Ω–¥–µ (–Ω–µ –¥–æ–≤–µ—Ä—è–µ–º AI –º–∞—Ç–µ–º–∞—Ç–∏–∫—É!)
        skills_analysis = result.get("skills_analysis", {})
        verified = len(skills_analysis.get("verified", []))
        unverified = len(skills_analysis.get("unverified", []))
        missing = len(skills_analysis.get("missing", []))
        total_required = verified + missing

        if total_required > 0:
            # Verified = 100%, Unverified = 50% –≤–µ—Å–∞
            match_score = verified + (unverified * 0.5)
            result["skills_match"] = int((match_score / total_required) * 100)
        else:
            result["skills_match"] = 50

        result["experience_match"] = int(scores.get("relevance", 3) * 20)

        # –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        result["matching_skills"] = skills_analysis.get("verified", [])
        result["skill_gaps"] = skills_analysis.get("missing", [])
        result["strengths"] = result.get("pros", [])
        result["weaknesses"] = result.get("cons", [])
        result["summary_one_line"] = result.get("summary_for_recruiter", "")

        # career_trajectory
        trajectory_score = scores.get("trajectory", 3)
        exp_summary = result.get("experience_summary", {})
        result["career_trajectory"] = exp_summary.get("career_direction", "stable")

        # Tier-1 –∫–æ–º–ø–∞–Ω–∏–∏
        result["tier1_companies"] = exp_summary.get("tier1_companies", [])

        return result

    async def analyze_resume(
        self,
        vacancy_data: Dict[str, Any],
        resume_data: Dict[str, Any],
        force_reanalysis: bool = False
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ v3.0"""
        start_time = time.time()

        try:
            cache_key = self._get_cache_key(vacancy_data, resume_data)

            if not force_reanalysis:
                cached_result = self._get_cached_analysis(cache_key)
                if cached_result:
                    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ª—É—á–µ–Ω –∏–∑ –∫–µ—à–∞: {cache_key}")
                    return cached_result

            prompt = self._create_analysis_prompt(vacancy_data, resume_data)

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–µ–∫—Ä—É—Ç–µ—Ä. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—é–º–µ –¥–µ—Ç–∞–ª—å–Ω–æ –∏ —á–µ—Å—Ç–Ω–æ.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON. –ù–µ –¥–æ–±–∞–≤–ª—è–π markdown –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω: —Ü–∏—Ç–∏—Ä—É–π —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–µ–∑—é–º–µ, —É–∫–∞–∑—ã–≤–∞–π —Ü–∏—Ñ—Ä—ã –∏ –∫–æ–º–ø–∞–Ω–∏–∏."""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2500,
                response_format={"type": "json_object"}
            )

            ai_response = response.choices[0].message.content

            try:
                analysis_result = json.loads(ai_response)
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                logger.error(f"–û—Ç–≤–µ—Ç AI: {ai_response[:500]}")
                raise AIAnalysisError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI")

            analysis_result = self._enrich_analysis_result(analysis_result)

            processing_time = int((time.time() - start_time) * 1000)
            analysis_result.update({
                "ai_model": self.model,
                "ai_tokens_used": response.usage.total_tokens,
                "ai_cost_rub": self._calculate_cost(response.usage.total_tokens),
                "processing_time_ms": processing_time,
                "prompt_version": "3.0"
            })

            self._cache_analysis(cache_key, analysis_result)

            logger.info(
                f"AI –∞–Ω–∞–ª–∏–∑ v3.0 –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time}ms, "
                f"—Ç–æ–∫–µ–Ω–æ–≤: {response.usage.total_tokens}, "
                f"verdict: {analysis_result.get('verdict', '?')}, "
                f"tier: {analysis_result.get('tier', '?')}"
            )

            return analysis_result

        except openai.RateLimitError:
            raise AIAnalysisError("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI API")

        except openai.APIError as e:
            logger.error(f"–û—à–∏–±–∫–∞ OpenAI API: {e}")
            raise AIAnalysisError(f"–û—à–∏–±–∫–∞ AI —Å–µ—Ä–≤–∏—Å–∞: {e}")

        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
            raise AIAnalysisError(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ: {e}")

    async def analyze_batch(
        self,
        vacancy_data: Dict[str, Any],
        resumes_data: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ"""
        if len(resumes_data) > 10:
            raise AIAnalysisError("–ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—é–º–µ –∑–∞ —Ä–∞–∑")

        import asyncio

        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_single(resume_data: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                try:
                    return await self.analyze_resume(vacancy_data, resume_data)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ {resume_data.get('id', 'unknown')}: {e}")
                    return {
                        "error": str(e),
                        "resume_id": resume_data.get("id"),
                        "status": "failed"
                    }

        tasks = [analyze_single(resume) for resume in resumes_data]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        successful_results = [
            result for result in results
            if isinstance(result, dict) and "error" not in result
        ]

        logger.info(f"–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {len(successful_results)}/{len(resumes_data)} —É—Å–ø–µ—à–Ω–æ")

        return successful_results

    def _calculate_cost(self, total_tokens: int) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Ä—É–±–ª—è—Ö"""
        # GPT-4o —Å—Ç–æ–∏–º–æ—Å—Ç—å (–ø—Ä–∏–º–µ—Ä–Ω–æ $2.5 input + $10 output –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤)
        # –°—Ä–µ–¥–Ω—è—è ~$5 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
        cost_per_1k_tokens_usd = 0.005
        cost_usd = (total_tokens / 1000) * cost_per_1k_tokens_usd
        cost_rub = cost_usd * 100  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –∫—É—Ä—Å
        return round(cost_rub, 2)

    async def get_analysis_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            "total_analyses": 0,
            "analyses_today": 0,
            "total_cost_rub": 0,
            "avg_score": 0,
            "cache_hit_rate": 0,
            "prompt_version": "3.0"
        }
